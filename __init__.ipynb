{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!:bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from nuimages.nuimages import NuImages          \n",
    "\n",
    "\n",
    "from train import train\n",
    "\n",
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 16\n",
    "NUM_EXAMPLES=1 \n",
    "FREEZE=False\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from nuimages.nuimages import NuImages\n",
    "from nuimages.utils.utils import name_to_index_mapping\n",
    "\n",
    "class NuImagesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataroot,\n",
    "                 version,\n",
    "                 desired_class_ids=None,    # e.g. [5,7,10]\n",
    "                 background_id=0,           # remapped value for all others\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 sensor_channels=None):\n",
    "        # --- 1) Initialize nuImages and filter key-frame tokens ---\n",
    "        self.nuim = NuImages(dataroot=dataroot,\n",
    "                             version=version,\n",
    "                             lazy=True,\n",
    "                             verbose=False)\n",
    "\n",
    "        # make channel lookup O(1)\n",
    "        self.sensor_channels = set(sensor_channels or\n",
    "                                   ['CAM_FRONT','CAM_FRONT_LEFT','CAM_FRONT_RIGHT'])\n",
    "\n",
    "        sd_tokens = []\n",
    "        for sd in self.nuim.sample_data:\n",
    "            if not sd['is_key_frame']:\n",
    "                continue\n",
    "            # get sensor channel via the calibrated_sensor → sensor join\n",
    "            cs = self.nuim.get('calibrated_sensor', sd['calibrated_sensor_token'])\n",
    "            channel = self.nuim.get('sensor', cs['sensor_token'])['channel']\n",
    "            if channel in self.sensor_channels:\n",
    "                sd_tokens.append(sd['token'])\n",
    "        self.sd_tokens = sd_tokens\n",
    "\n",
    "        self.transform        = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # --- 2) Build a one-time LUT for fast mask remapping ---\n",
    "        # get the full mapping of names→indices that get_segmentation uses\n",
    "        mapping = name_to_index_mapping(self.nuim.category)\n",
    "        max_idx = max(mapping.values())\n",
    "        lut = np.full((max_idx + 1,), fill_value=background_id, dtype=np.uint8)\n",
    "\n",
    "        # only keep your desired IDs\n",
    "        desired = set(desired_class_ids or [])\n",
    "        for cls_id in desired:\n",
    "            if 0 <= cls_id <= max_idx:\n",
    "                lut[cls_id] = cls_id\n",
    "\n",
    "        self.lut = lut\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sd_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sd_token = self.sd_tokens[idx]\n",
    "        sample_data = self.nuim.get('sample_data', sd_token)\n",
    "        img_path = os.path.join(self.nuim.dataroot,\n",
    "                                sample_data['filename'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # get raw sem‐seg mask (H×W int32 array)\n",
    "        sem_mask, _ = self.nuim.get_segmentation(sd_token)\n",
    "        raw = sem_mask.astype(np.uint8)\n",
    "\n",
    "        # single lookup = blazing fast C operation\n",
    "        remapped = self.lut[raw]\n",
    "\n",
    "        mask = Image.fromarray(remapped)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "resize_size = (256, 256)\n",
    "original_size = (1024, 2048)\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(resize_size, interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# Mask transforms (nearest-neighbor resize + to tensor)\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize(resize_size, interpolation=Image.NEAREST),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda t: t.squeeze(0).long()),\n",
    "])\n",
    "\n",
    "# root = os.getenv('NUIMAGES')\n",
    "root = '/var/tmp/MultiTask_vs_Yolo_Unet/full_nuImages'\n",
    "train_version = 'v1.0-train'\n",
    "val_version   = 'v1.0-val'\n",
    "\"\"\"\n",
    " 1 → animal\n",
    " 2 → human.pedestrian.adult\n",
    " 3 → human.pedestrian.child\n",
    " 4 → human.pedestrian.construction_worker\n",
    " 5 → human.pedestrian.personal_mobility\n",
    " 6 → human.pedestrian.police_officer\n",
    " 7 → human.pedestrian.stroller\n",
    " 8 → human.pedestrian.wheelchair\n",
    " 9 → movable_object.barrier\n",
    "10 → movable_object.debris\n",
    "11 → movable_object.pushable_pullable\n",
    "12 → movable_object.trafficcone\n",
    "13 → static_object.bicycle_rack\n",
    "14 → vehicle.bicycle\n",
    "15 → vehicle.bus.bendy\n",
    "16 → vehicle.bus.rigid\n",
    "17 → vehicle.car\n",
    "18 → vehicle.construction\n",
    "19 → vehicle.emergency.ambulance\n",
    "20 → vehicle.emergency.police\n",
    "21 → vehicle.motorcycle\n",
    "22 → vehicle.trailer\n",
    "23 → vehicle.truck\n",
    "24 → flat.driveable_surface\n",
    "31 → vehicle.ego\n",
    "\"\"\"\n",
    "desired_ids =  [9, 10, 11, 12, 13, 24]\n",
    "# Datasets\n",
    "train_dataset = NuImagesDataset(root, train_version, desired_class_ids=desired_ids)\n",
    "                                # transform=input_transform,\n",
    "                                # target_transform=target_transform)\n",
    "\n",
    "val_dataset   = NuImagesDataset(root, val_version)\n",
    "                                # transform=input_transform,\n",
    "                                # target_transform=target_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset,\n",
    "#                           batch_size=BATCH_SIZE,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=NUM_WORKERS)\n",
    "\n",
    "# val_loader   = DataLoader(val_dataset,\n",
    "#                           batch_size=BATCH_SIZE,\n",
    "#                           shuffle=False,\n",
    "#                           num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "# for images, masks in train_loader:\n",
    "#     print(images.shape)  # torch.Size([4, 3, H, W])\n",
    "#     print(masks.shape)   # torch.Size([4, H, W])\n",
    "#     print(masks.dtype)   # torch.int64\n",
    "#     break\n",
    "\n",
    "\n",
    "indices = [3, 4, 5]\n",
    "\n",
    "for idx in indices:\n",
    "    image, mask = train_dataset[idx]\n",
    "    # image: PIL.Image RGB\n",
    "    # mask:  PIL.Image (mode 'L'), values [0..Nclasses]\n",
    "    img_arr = np.array(image)\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(img_arr)\n",
    "    # overlay mask in semi‐transparent red where mask>0\n",
    "    plt.imshow(mask_arr, alpha=0.4)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Sample {idx}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuimages.utils.utils import name_to_index_mapping# Build the mapping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from nuimages.nuimages import NuImages          \n",
    "\n",
    "dataroot = '/var/tmp/MultiTask_vs_Yolo_Unet/full_nuImages'\n",
    "version = 'v1.0-train'\n",
    "val_version   = 'v1.0-val'\n",
    "nuim = NuImages(dataroot=dataroot,\n",
    "                             version=version,\n",
    "                             lazy=True,\n",
    "                             verbose=False)\n",
    "mapping = name_to_index_mapping(nuim.category)\n",
    "\n",
    "# Display it\n",
    "for name, idx in mapping.items():\n",
    "    print(f\"{idx:2d} → {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd960fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(nuim_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = [3, 4, 5]\n",
    "\n",
    "for idx in indices:\n",
    "    image, mask = train_dataset[idx]\n",
    "    # image: PIL.Image RGB\n",
    "    # mask:  PIL.Image (mode 'L'), values [0..Nclasses]\n",
    "    img_arr = np.array(image)\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img_arr)\n",
    "    # overlay mask in semi‐transparent red where mask>0\n",
    "    plt.imshow(np.where(mask_arr>0, 1, np.nan), \n",
    "               cmap='Reds', alpha=0.4, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Sample {idx}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d70564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "39.93558478355408\n",
      "40.05818200111389\n",
      "40.119895458221436\n",
      "40.1733193397522\n",
      "40.6914758682251\n",
      "41.00242018699646\n",
      "41.005342960357666\n",
      "41.00759935379028\n",
      "45.8759081363678\n",
      "45.88336133956909\n",
      "45.97970390319824\n",
      "46.002256870269775\n",
      "46.57042932510376\n",
      "46.640608072280884\n",
      "46.64455056190491\n",
      "46.64609408378601\n",
      "51.25409913063049\n",
      "51.75851774215698\n",
      "52.165422439575195\n",
      "52.16622614860535\n",
      "52.16653823852539\n",
      "52.61426830291748\n",
      "52.61571526527405\n",
      "52.618473291397095\n",
      "56.49860191345215\n",
      "57.42618656158447\n",
      "57.445735692977905\n",
      "58.53869390487671\n",
      "58.53903317451477\n",
      "58.5398154258728\n",
      "58.540103912353516\n",
      "58.541067361831665\n",
      "62.511783838272095\n",
      "62.51215100288391\n",
      "62.64790487289429\n",
      "64.24093866348267\n",
      "64.24178266525269\n",
      "64.24202299118042\n",
      "64.24286890029907\n",
      "64.2453989982605\n",
      "68.04193067550659\n",
      "68.0483787059784\n",
      "68.04879093170166\n",
      "69.56587600708008\n",
      "69.56619095802307\n",
      "70.03074359893799\n",
      "70.0311176776886\n",
      "70.0316903591156\n",
      "74.16999340057373\n",
      "74.1706440448761\n",
      "74.17082524299622\n",
      "74.58752846717834\n",
      "74.58821034431458\n",
      "75.05122113227844\n",
      "75.05162835121155\n",
      "75.05186581611633\n",
      "79.50353765487671\n",
      "79.73066067695618\n",
      "79.73116946220398\n",
      "79.87059211730957\n",
      "79.87127470970154\n",
      "80.54643678665161\n",
      "80.5471818447113\n",
      "80.54848051071167\n",
      "85.38163995742798\n",
      "85.63087916374207\n",
      "85.63247609138489\n",
      "85.99771094322205\n",
      "86.0004711151123\n",
      "87.09300827980042\n",
      "87.09384441375732\n",
      "87.09548592567444\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from nuimages.nuimages import NuImages          \n",
    "\n",
    "\n",
    "from train import train\n",
    "from typing import Optional\n",
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 16\n",
    "NUM_EXAMPLES=1 \n",
    "FREEZE=False\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from nuimages.nuimages import NuImages\n",
    "from nuimages.utils.utils import name_to_index_mapping\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from nuimages.nuimages import NuImages\n",
    "from nuimages.utils.utils import name_to_index_mapping\n",
    "\n",
    "IGNORE_INDEX = 255\n",
    "\n",
    "class NuImagesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataroot,\n",
    "                 version,\n",
    "                 desired_class_ids:\n",
    "                   Optional[list]=None,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 sensor_channels=None):\n",
    "\n",
    "        self.nuim = NuImages(dataroot=dataroot,\n",
    "                             version=version,\n",
    "                             lazy=True, verbose=False)\n",
    "        cameras = set(sensor_channels or\n",
    "                      ['CAM_FRONT','CAM_FRONT_LEFT','CAM_FRONT_RIGHT'])\n",
    "        self.sd_tokens = [\n",
    "            sd['token']\n",
    "            for sd in self.nuim.sample_data\n",
    "            if sd['is_key_frame']\n",
    "            and self.nuim.shortcut('sample_data','sensor',sd['token'])['channel']\n",
    "                in cameras\n",
    "        ]\n",
    "        self.transform        = transform\n",
    "        self.target_transform = target_transform\n",
    "        name2idx = name_to_index_mapping(self.nuim.category)\n",
    "        max_old   = max(name2idx.values())\n",
    "        desired = desired_class_ids or []\n",
    "        self.num_classes = 1 + len(desired)\n",
    "        lut = np.full((max_old+1,), fill_value=IGNORE_INDEX, dtype=np.uint8)\n",
    "        lut[0] = 0\n",
    "        for new_id, old_id in enumerate(desired, start=1):\n",
    "            lut[old_id] = new_id\n",
    "\n",
    "        self.lut = lut\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sd_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sd_token = self.sd_tokens[idx]\n",
    "        sd       = self.nuim.get('sample_data', sd_token)\n",
    "        img_path = os.path.join(self.nuim.dataroot, sd['filename'])\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "        sem_mask, _ = self.nuim.get_segmentation(sd_token)\n",
    "        raw         = sem_mask.astype(np.uint8)\n",
    "        remapped = self.lut[raw] \n",
    "        mask = Image.fromarray(remapped)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "resize_size = (256, 256)\n",
    "original_size = (1024, 2048)\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(resize_size, interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# Mask transforms (nearest-neighbor resize + to tensor)\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize(resize_size, interpolation=Image.NEAREST),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda t: t.squeeze(0).long()),\n",
    "])\n",
    "\n",
    "# root = os.getenv('NUIMAGES')\n",
    "root = '/var/tmp/MultiTask_vs_Yolo_Unet/full_nuImages'\n",
    "train_version = 'v1.0-train'\n",
    "val_version   = 'v1.0-val'\n",
    "\n",
    "desired_ids =  [9, 10, 11, 12, 13, 24]\n",
    "# Datasets\n",
    "train_dataset = NuImagesDataset(root, train_version, desired_class_ids=desired_ids,\n",
    "                                transform=input_transform,\n",
    "                                target_transform=target_transform)\n",
    "\n",
    "val_dataset   = NuImagesDataset(root, val_version, desired_class_ids=desired_ids,\n",
    "                                transform=input_transform,\n",
    "                                target_transform=target_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "val_loader   = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "nuim_train = NuImages(dataroot=root, version=train_version, lazy=True, verbose=False)\n",
    "num_classes = len(desired_ids)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "from train import train\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    total_steps=len(train_loader)*NUM_EPOCHS,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      train_loader=train_loader,\n",
    "      val_loader=val_loader,\n",
    "      num_epochs=NUM_EPOCHS,\n",
    "      num_classes=num_classes,\n",
    "      scheduler=scheduler,\n",
    "      freeze_encoder=False, \n",
    "      plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a302a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuim_train = NuImages(dataroot=root, version=train_version, lazy=True, verbose=False)\n",
    "[cat[\"name\"] for cat in nuim_train.category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41897536",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nuim.sample_data[0]\n",
    "nuim.shortcut('sample_data', '', st['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9913f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading nuImages tables for version v1.0-train...\n",
      "Done loading in 0.000 seconds (lazy=True).\n",
      "======\n",
      "Loaded 67279 sample(s) in 0.073s,\n",
      "Loaded 872181 sample_data(s) in 2.742s,\n",
      "Loaded 557715 object_ann(s) in 3.312s,\n",
      "Loaded 25 category(s) in 0.000s,\n",
      "Filtered annotation image saved to /var/tmp/MultiTask_vs_Yolo_Unet/full_nuImages/v1.0-train/first_sample/first_sample_filtered.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "from nuimages.nuimages import NuImages\n",
    "\n",
    "# Initialize nuImages\n",
    "dataroot = '/var/tmp/MultiTask_vs_Yolo_Unet/full_nuImages'\n",
    "version = 'v1.0-train'\n",
    "nuim = NuImages(dataroot=dataroot, version=version, verbose=True)\n",
    "\n",
    "# Get a sample\n",
    "sample = nuim.sample[50]\n",
    "key_camera_token = sample['key_camera_token']\n",
    "sample_data = nuim.get('sample_data', key_camera_token)\n",
    "image_path = os.path.join(nuim.dataroot, sample_data['filename'])\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(image_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "# Define which categories to keep\n",
    "#  - exactly 'vehicle.car'\n",
    "#  - anything that starts with 'human.pedestrian'\n",
    "#  - exactly 'animal'\n",
    "def keep_category(name: str) -> bool:\n",
    "    return (\n",
    "        name == 'vehicle.car' or\n",
    "        name == 'animal' or\n",
    "        name.startswith('human.pedestrian')\n",
    "    )\n",
    "\n",
    "# Fetch and filter annotations for this frame\n",
    "filtered_anns = []\n",
    "for ann in nuim.object_ann:\n",
    "    if ann['sample_data_token'] != key_camera_token:\n",
    "        continue\n",
    "    cat = nuim.get('category', ann['category_token'])['name']\n",
    "    if keep_category(cat):\n",
    "        filtered_anns.append((ann, cat))\n",
    "\n",
    "# Draw only the filtered boxes\n",
    "for ann, category in filtered_anns:\n",
    "    xmin, ymin, xmax, ymax = map(int, ann['bbox'])\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        category,\n",
    "        (xmin, max(0, ymin - 10)),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.5,\n",
    "        (0, 255, 0),\n",
    "        1,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "\n",
    "# Save result\n",
    "output_dir = os.path.join(dataroot, version, 'first_sample')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "out_path = os.path.join(output_dir, 'first_sample_filtered.png')\n",
    "cv2.imwrite(out_path, img)\n",
    "print(f\"Filtered annotation image saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/devdem/MultiTask_vs_Yolo_Unet/HybridNets')  # замените на реальный абсолютный путь до папки HybridNets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6676353a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hybridnets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnuimages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnuimages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NuImages\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhybridnets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhybridnets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HybridNetsBackbone\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhybridnets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcriterion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiTaskLoss\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhybridnets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pretrained\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hybridnets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Any, Tuple\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "from nuimages.nuimages import NuImages\n",
    "from hybridnets.model.hybridnets import HybridNetsBackbone\n",
    "from hybridnets.utils.criterion import MultiTaskLoss\n",
    "from hybridnets.utils.utils import load_pretrained\n",
    "\n",
    "# ----------------------\n",
    "# Utility: plot losses\n",
    "# ----------------------\n",
    "def plot_losses(train_losses: List[float], val_losses: List[float]):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
    "    ax[0].plot(range(1, len(val_losses) + 1), val_losses, label='val')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "\n",
    "    ax[1].plot(range(1, len(train_losses) + 1), np.exp(train_losses), label='train')\n",
    "    ax[1].plot(range(1, len(val_losses) + 1), np.exp(val_losses), label='val')\n",
    "    ax[1].set_title('Perplexity')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------\n",
    "# Dataset for NuImages + HybridNets\n",
    "# ------------------------------------\n",
    "class NuImagesHybridDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataroot: str,\n",
    "                 version: str,\n",
    "                 transform: Optional[Any] = None,\n",
    "                 target_size: Tuple[int, int] = (320, 640)):\n",
    "        self.nuim = NuImages(dataroot=dataroot, version=version, lazy=True, verbose=False)\n",
    "        self.sd_tokens = [\n",
    "            sd['token'] for sd in self.nuim.sample_data\n",
    "            if sd['is_key_frame'] and\n",
    "               self.nuim.shortcut('sample_data', 'sensor', sd['token'])['channel'] == 'CAM_FRONT'\n",
    "        ]\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.category_to_id = {cat['name']: i for i, cat in enumerate(self.nuim.category)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sd_tokens)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        tok = self.sd_tokens[idx]\n",
    "        sd = self.nuim.get('sample_data', tok)\n",
    "        img_file = os.path.join(self.nuim.dataroot, sd['filename'])\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "        orig_w, orig_h = img.size\n",
    "        new_h, new_w = self.target_size\n",
    "\n",
    "        # Segmentation mask\n",
    "        sem_mask, _ = self.nuim.get_segmentation(tok)\n",
    "        seg = Image.fromarray(sem_mask.astype(np.uint8))\n",
    "\n",
    "        # Object annotations\n",
    "        anns = [ann for ann in self.nuim.object_ann if ann['sample_data_token'] == tok]\n",
    "        bboxes, labels = [], []\n",
    "        for ann in anns:\n",
    "            xmin, ymin, xmax, ymax = ann['bbox']\n",
    "            cx = (xmin + xmax) / 2 / orig_w\n",
    "            cy = (ymin + ymax) / 2 / orig_h\n",
    "            bw = (xmax - xmin) / orig_w\n",
    "            bh = (ymax - ymin) / orig_h\n",
    "            cat = self.nuim.get('category', ann['category_token'])['name']\n",
    "            cls_id = self.category_to_id[cat]\n",
    "            bboxes.append([cx, cy, bw, bh])\n",
    "            labels.append(cls_id)\n",
    "\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Lane placeholder\n",
    "        lane = Image.fromarray(np.zeros((orig_h, orig_w), dtype=np.uint8))\n",
    "\n",
    "        # Transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            seg = self.transform(seg)\n",
    "            lane = self.transform(lane)\n",
    "\n",
    "        # seg: [1,H,W], lane: [1,H,W]\n",
    "        return img, seg.squeeze(0).long(), (bboxes, labels), lane.squeeze(0).long()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Training / Validation Loops (HybridNets version)\n",
    "# ------------------------------------------------\n",
    "def training_epoch(model: nn.Module,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   criterion: MultiTaskLoss,\n",
    "                   loader: DataLoader,\n",
    "                   desc: str) -> float:\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for imgs, segs, (bboxes, lbls), lanes in tqdm(loader, desc=desc):\n",
    "        imgs, segs, lanes = imgs.to(device), segs.to(device), lanes.to(device)\n",
    "        # detection targets stay on CPU for criterion\n",
    "        optimizer.zero_grad()\n",
    "        out_seg, out_det, out_lane = model(imgs)\n",
    "        loss, _ = criterion(\n",
    "            out_seg, segs,\n",
    "            out_det, bboxes, lbls,\n",
    "            out_lane, lanes\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model: nn.Module,\n",
    "                     criterion: MultiTaskLoss,\n",
    "                     loader: DataLoader,\n",
    "                     desc: str) -> float:\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for imgs, segs, (bboxes, lbls), lanes in tqdm(loader, desc=desc):\n",
    "        imgs, segs, lanes = imgs.to(device), segs.to(device), lanes.to(device)\n",
    "        out_seg, out_det, out_lane = model(imgs)\n",
    "        loss, _ = criterion(\n",
    "            out_seg, segs,\n",
    "            out_det, bboxes, lbls,\n",
    "            out_lane, lanes\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "    return val_loss / len(loader)\n",
    "\n",
    "# ----------------------\n",
    "# Main training script\n",
    "# ----------------------\n",
    "def main():\n",
    "    # Hyperparams\n",
    "    DATAROOT = '/var/tmp/nuImages'\n",
    "    VERSION = 'v1.0-mini'\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 2\n",
    "    NUM_EPOCHS = 5\n",
    "    LR = 1e-4\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Dataset & Loader\n",
    "    transform = T.Compose([\n",
    "        T.Resize((320, 640)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    train_ds = NuImagesHybridDataset(DATAROOT, VERSION, transform)\n",
    "    val_ds   = NuImagesHybridDataset(DATAROOT, VERSION, transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Model & Loss\n",
    "    num_classes = len(train_ds.category_to_id)\n",
    "    num_lanes = 1\n",
    "    model = HybridNetsBackbone(num_classes=num_classes, num_lanes=num_lanes).to(device)\n",
    "    load_pretrained(model)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = MultiTaskLoss().to(device)\n",
    "\n",
    "    # Training\n",
    "    best_val = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_loss = training_epoch(model, optimizer, criterion, train_loader, f\"Train {epoch}/{NUM_EPOCHS}\")\n",
    "        val_loss   = validation_epoch(model, criterion, val_loader, f\"Val   {epoch}/{NUM_EPOCHS}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), 'hybridnets_best.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), 'hybridnets_last.pth')\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
